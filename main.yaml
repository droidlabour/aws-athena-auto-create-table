AWSTemplateFormatVersion: 2010-09-09

Description: Amazon Athena

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
    - Label:
        default: Athena Settings
      Parameters:
      - InputBucketName
      - AthenaDBName
    - Label:
        default: IAM Credentials Report
      Parameters:
      - IamReportBucketName
      - IamReportBucketFolder

Parameters:
  InputBucketName:
    Type: String
    Description: Input S3 Bucket Name where files will be uploaded for Athena
  AthenaDBName:
    Type: String
    AllowedPattern: ^[a-z]*$
    Description: Athena Database name, will be created if it does not exist
  UpdateMarker:
    Type: String
    Default: 1.0
    Description: Leave this empty, this is just for debugging purpose.
  IamReportBucketName:
    Type: String
    Description: S3 Bucket where IAM credential report will be uploaded
  IamReportBucketFolder:
    Type: String
    Description: S3 Bucket folder where IAM credential report will be uploaded

Resources:
  InputBucket:
    Type: AWS::S3::Bucket
    DependsOn: LambdaInvokePermission
    Properties:
      BucketName: !Ref InputBucketName
      LifecycleConfiguration:
        Rules:
        - Id: GlacierRule
          Status: Enabled
          TagFilters:
            - Key: Type
              Value: AthenaDataSet
          Transitions:
            - TransitionInDays: '30'
              StorageClass: GLACIER
      NotificationConfiguration:
        LambdaConfigurations:
          -
            Function: !GetAtt CreateTableFunction.Arn
            Event: "s3:ObjectCreated:*"
            Filter:
              S3Key:
                Rules:
                  -
                    Name: suffix
                    Value: csv
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref CreateTableFunction
      Principal: s3.amazonaws.com
      SourceArn: !Sub arn:aws:s3:::${InputBucketName}
  OutputBucket:
    Type: AWS::S3::Bucket
  CreateTableFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import re
          import json
          import traceback
          from time import sleep
          from urllib.parse import unquote
          
          import boto3
          
          athena = boto3.client('athena')
          s3 = boto3.client('s3')
          dbName = os.getenv('AthenaDbName')
          oputBucket = os.getenv('OutputBucket')
          createTable = """
          CREATE EXTERNAL TABLE IF NOT EXISTS
            {}.{} {}
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES ({})
            LOCATION '{}'
            TBLPROPERTIES ('skip.header.line.count'='1');
          """
          
          
          def queryStatus(qid):
              return athena.get_query_execution(QueryExecutionId=qid)['QueryExecution']['Status']['State']
          
          
          def createColumns(columns):
              x = []
              for i in columns:
                  i = i.strip().lower()
                  i = re.sub('^"', '', i)
                  i = re.sub('"$', '', i)
                  x.append('`' + re.sub('[^a-z0-9-]+', '-', i) + '` string')
              return x
          
          
          def wait4Query(qid):
              while queryStatus(qid) in ['QUEUED', 'RUNNING']:
                  print("Waiting query id to finish {}".format(qid))
                  sleep(5)
              print("query id finished {}".format(qid))
          
          
          def run_query(query):
              print("Running query: {}".format(query))
              qid = athena.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={'Database': dbName},
                  ResultConfiguration={'OutputLocation': oputBucket}
              )['QueryExecutionId']
              print("query id: {}".format(qid))
              return qid
          
          
          def handler(event, context):
              print(json.dumps(event))
          
              try:
                  query = "CREATE DATABASE IF NOT EXISTS {};".format(dbName)
                  query_id = run_query(query)
                  wait4Query(query_id)
          
                  for r in event['Records']:
                      bucket = r['s3']['bucket']['name']
                      key = r['s3']['object']['key'].replace('+', ' ')
                      if 'view' in key:  # Skip for files under view dir
                          continue
                      s3.put_object_tagging(
                          Bucket=bucket,
                          Key=key,
                          Tagging={
                              'TagSet': [
                                  {
                                      'Key': 'Type',
                                      'Value': 'AthenaDataSet'
                                  }
                              ]
                          }
                      )
                      csv_file = '/tmp/' + os.path.basename(key)
                      csv_path = os.path.dirname(key)
                      table_name = re.sub('[^a-z0-9_]+', '_', csv_path.split('/')[-1].lower())
                      location = 's3://{}/{}/'.format(bucket, csv_path)
                      s3.download_file(bucket, unquote(key), csv_file)
                      columns = []
                      serde_prop = ''
                      with open(csv_file, 'r') as f:
                          _ = f.readline()
                          a = _.split('|')
                          b = _.split(',')
                          if len(a) > len(b):
                              serde_prop = "'separatorChar' = '|', 'serialization.format' = ',', 'field.delim' = '|'"
                              columns = createColumns(a)
                          else:
                              serde_prop = "'serialization.format' = ',', 'field.delim' = ','"
                              columns = createColumns(b)
                      columns = '(' + ', ' . join(columns) + ')'
          
                      query = createTable.format(dbName, table_name, columns, serde_prop, location)
                      query_id = run_query(query)
                      wait4Query(query_id)
          
                      x = csv_path.split('/')[:-1]
                      x.append('views/')
                      view_path = '/' . join(x)
                      files = s3.list_objects(Bucket=bucket, Prefix=view_path)
                      if 'Contents' in files.keys():
                          for k, f in enumerate(files['Contents']):
                              if f['Key'].endswith('/'):
                                  continue
                              view_file = '/tmp/' + os.path.basename(f['Key'])
                              s3.download_file(bucket, f['Key'], view_file)
                              with open(view_file, 'r') as v:
                                  view_name = table_name + '_' + str(k) + '_view'
                                  query = v.read().format(view_name, table_name)
                                  query_id = run_query(query)
                                  wait4Query(query_id)
              except Exception as e:
                  print(str(e))
                  traceback.print_exc()
              return 0
      Handler: index.handler
      Environment:
        Variables:
          AthenaDbName: !Ref AthenaDBName
          OutputBucket: !Sub s3://${OutputBucket}/
      FunctionName: Quicksight-Athena-Function
      Role: !GetAtt CreateTableRole.Arn
      Runtime: python3.7
      Timeout: 300
  CreateTableRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: CreateTablePolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: s3:*
            Resource: '*'
          - Effect: Allow
            Action: athena:*
            Resource: '*'
          - Effect: Allow
            Action: glue:*
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
  AddTagsToS3ObjectsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import json
          import fnmatch

          import boto3

          import cfnresponse

          s3 = boto3.client('s3')
          status = cfnresponse.SUCCESS

          def handler(event, context):
              print(json.dumps(event))
              print(context)
              if event['RequestType'] == 'Delete':
                  cfnresponse.send(event, context, status, {}, None)
                  return 0
              bucket = event['ResourceProperties']['InputBucket']
              p1 = s3.get_paginator('list_objects_v2')
              p2 = p1.paginate(Bucket=bucket)
              
              for i in p2:
                  if 'Contents' in i.keys():
                      for k in i['Contents']:
                          if fnmatch.fnmatch(k['Key'], '*.csv') and not fnmatch.fnmatch(k['Key'], '*/views/*'):
                              key = k['Key'].replace('+', ' ')
                              print('Tagging key ' + key)
                              s3.put_object_tagging(
                                  Bucket=bucket,
                                  Key=key,
                                  Tagging={
                                      'TagSet': [
                                          {
                                              'Key': 'Type',
                                              'Value': 'AthenaDataSet'
                                          }
                                      ]
                                  }
                              )
              cfnresponse.send(event, context, status, {}, None)
              return 0
      Handler: index.handler
      FunctionName: Add-Tags-To-S3Objects-Function
      Role: !GetAtt AddTagsToS3ObjectsRole.Arn
      Runtime: python3.7
      Timeout: 300
  AddTagsToS3ObjectsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: AddTagsToS3ObjectsPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: s3:*
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
  AddTagsToS3Objects:
    Type: AWS::CloudFormation::CustomResource
    DependsOn: InputBucket
    Properties:
      ServiceToken: !GetAtt 'AddTagsToS3ObjectsFunction.Arn'
      UpdateMarker: !Ref UpdateMarker
      InputBucket: !Ref InputBucketName
  DeleteOldAthenaDbViewFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import re
          import json
          import traceback
          from time import sleep
          from datetime import datetime
          
          import boto3
          
          athena = boto3.client('athena')
          dbName = os.getenv('AthenaDbName')
          oputBucket = os.getenv('OutputBucket')
          
          
          def queryStatus(qid):
              return athena.get_query_execution(QueryExecutionId=qid)['QueryExecution']['Status']['State']
          
          
          def getQueryResult(qid):
              result = athena.get_query_results(QueryExecutionId=qid)
              if 'ResultSet' in result.keys():
                  if 'Rows' in result['ResultSet'].keys():
                      return result['ResultSet']['Rows']
              return []
          
          
          def wait4Query(qid):
              while queryStatus(qid) in ['QUEUED', 'RUNNING']:
                  print("Waiting query id to finish {}".format(qid))
                  sleep(5)
              print("query id finished {}".format(qid))
              return qid
          
          
          def run_query(query):
              print("Running query: {}".format(query))
              qid = athena.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={'Database': dbName},
                  ResultConfiguration={'OutputLocation': oputBucket}
              )['QueryExecutionId']
              print("query id: {}".format(qid))
              return qid
          
          
          def handler(event, context):
              print(json.dumps(event))
              epocRegex = r'\t(\d.+)$'
          
              try:
                  query = 'SHOW TABLES;'
                  tableResults = getQueryResult(wait4Query(run_query(query)))
                  for tableResult in tableResults:
                      table = tableResult['Data'][0]['VarCharValue']
                      print("Table: {}".format(table))
                      query = "SHOW TBLPROPERTIES {}('transient_lastDdlTime');".format(table)
                      property = getQueryResult(wait4Query(run_query(query)))
                      if not property:
                          continue
                      property = property[0]['Data'][0]['VarCharValue']
          
                      tableCreatedAt = re.findall(epocRegex, property)
                      if tableCreatedAt:
                          print("Table: {} created at: {}".format(table, tableCreatedAt[0]))
                          if (datetime.utcnow() - datetime.utcfromtimestamp(int(tableCreatedAt[0]))).days > 30:
                              print("Table: {} older then 30 days".format(table))
                              print("Deleting Table: {}".format(table))
                              run_query("DROP TABLE IF EXISTS {};".format(table))
                              query = "SHOW VIEWS LIKE '{}*_view';".format(table)
                              for views in getQueryResult(wait4Query(run_query(query))):
                                  print("Deleting Views: {}".format(views['Data'][0]['VarCharValue']))
                                  run_query("DROP VIEW IF EXISTS {};".format(views['Data'][0]['VarCharValue']))
              except Exception as e:
                  print(str(e))
                  traceback.print_exc()
              return 0
      Handler: index.handler
      Environment:
        Variables:
          AthenaDbName: !Ref AthenaDBName
          OutputBucket: !Sub s3://${OutputBucket}/
      FunctionName: Delete-Old-Athena-Db-View-Function
      Role: !GetAtt DeleteOldAthenaDbViewRole.Arn
      Runtime: python3.7
      Timeout: 900
  DeleteOldAthenaDbViewRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: DeleteOldAthenaDbViewPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: s3:*
            Resource: '*'
          - Effect: Allow
            Action: athena:*
            Resource: '*'
          - Effect: Allow
            Action: glue:*
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
  DeleteOldAthenaDbViewLambdaCron:
    Properties:
      Description: Clean Athena tables & views older then 30 days
      ScheduleExpression: "rate(1 day)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt DeleteOldAthenaDbViewFunction.Arn
          Id: Delete-Old-Athena-Db-View-Cron
    Type: AWS::Events::Rule
  EventPermissionToInvokeDeleteOldAthenaDbViewLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Ref: DeleteOldAthenaDbViewFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn:
        Fn::GetAtt:
          - "DeleteOldAthenaDbViewLambdaCron"
          - "Arn"
  IamCredentialsReportFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import json
          from time import sleep
          from datetime import date
          
          import boto3
          
          iam = boto3.client('iam')
          s3 = boto3.client('s3')
          s3Bucket = os.getenv('IamReportBucketName')
          s3Dir = os.getenv('IamReportBucketFolder')
          
          
          def handler(event, context):
              print(json.dumps(event))
              while iam.generate_credential_report()['State'] != 'COMPLETE':
                  print('Waiting for generate_credential_report to finish')
                  sleep(5)

              f=os.path.normpath(s3Dir).strip('/') + '/IAM_credential_report' + ' ' + date.today().strftime('%B %d %Y') + '.csv'
              print('Uploading credentials report to S3 file => ' + f)
              s3.put_object(
                  Bucket=s3Bucket,
                  Key=f,
                  Body=iam.get_credential_report()['Content']
              )
              return 0
      Handler: index.handler
      Environment:
        Variables:
          IamReportBucketName: !Ref IamReportBucketName
          IamReportBucketFolder: !Ref IamReportBucketFolder
      FunctionName: Iam-Credentials-Report-Function
      Role: !GetAtt IamCredentialsReportRole.Arn
      Runtime: python3.7
      Timeout: 300
  IamCredentialsReportRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: IamCredentialsReportPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: s3:*
            Resource: '*'
          - Effect: Allow
            Action: iam:*
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*
  IamCredentialsReportLambdaCron:
    Properties:
      Description: Generate IAM Credentials Report
      ScheduleExpression: "rate(1 day)"
      State: ENABLED
      Targets:
        - Arn: !GetAtt IamCredentialsReportFunction.Arn
          Id: Iam-Credentials-Report-Cron
    Type: AWS::Events::Rule
  EventPermissionToInvokeIamCredentialsReportLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Ref: IamCredentialsReportFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn:
        Fn::GetAtt:
          - "IamCredentialsReportLambdaCron"
          - "Arn"


Outputs:
  OutputLocation:
    Value: !Sub s3://${OutputBucket}/
